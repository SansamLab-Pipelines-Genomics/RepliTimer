configfile: "config/config.yml"

##################################################################
##                   import samples .csv file                   ##
##################################################################

# this imports the pandas package functionality in an object named pd
import pandas as pd

# this reads the CSV file and sets an index using the values in the "sample" column.
samples_table = pd.read_csv(config["samples_table"]).set_index("sample", drop=False)

# fastq filename input function definition set to Python dictionary
def fq_dict_from_sample(wildcards):
  return {
    "fq1": samples_table.loc[wildcards.sample, "fastq1"],
    "fq2": samples_table.loc[wildcards.sample, "fastq2"]
  }

#################################################################
##                     Define target files                     ##
#################################################################

# to run snakemake without explicitly requesting any output files on the command line, we must request output files in the first rule. Therefore we include this otherwise useless rule here  
rule all:
    input:
        expand("results/counts/{sample}_counts.bedgraph", sample = samples_table.index),
        expand("results/processed_rse/{sample}_processed_rse.rds", sample = config["experiment_name"]),
        expand("results/bedgraphs/{sample}_Log2Ratios_bedgraphs.gz",sample = config["experiment_name"]),
        expand("results/bedgraphs/{sample}_ZScores_bedgraphs.gz",sample = config["experiment_name"]),
        expand("results/bedgraphs/{sample}_Smoothed_bedgraphs.gz",sample = config["experiment_name"]),
        expand("results/bedgraphs/{sample}_Quotients_bedgraphs.gz",sample = config["experiment_name"])

##################################################################
##                          Trim reads                          ##
##################################################################

rule trim_reads_with_fastp:
    input:
        unpack(fq_dict_from_sample)   # <--- we need to wrap our input function inside a special Snakemake function called unpack() which turns the dict into a collection of named inputs
    params:
        threads=config["fastp_cpus"]
    output:
        trimmed1="results/trimmed/{sample}_trimmed_R1.fastq.gz",
        trimmed2="results/trimmed/{sample}_trimmed_R2.fastq.gz",
        fastp_report="results/qc/fastp_reports/{sample}.html",
        fastp_json="results/qc/fastp_reports/{sample}.json"
    envmodules:
        "fastp/0.23.2"
    conda:
        "workflow/envs/RTEnv.yml"
    log: "results/logs/snakelogs/trim_reads_with_fastp.{sample}.log"
    shell:
        """
        fastp -i {input.fq1} -I {input.fq2} -o {output.trimmed1} -O {output.trimmed2} -h {output.fastp_report} --json {output.fastp_json} -R "{wildcards.sample}" -w {params.threads}
        """

#################################################################
##                    Align reads to genome                    ##
#################################################################

rule align_reads_with_bwamem:
    input:
        R1="results/trimmed/{sample}_trimmed_R1.fastq.gz",
        R2="results/trimmed/{sample}_trimmed_R2.fastq.gz"
    params:
        genome=config["bwa_genome"],
        threads=config["bwa_cpus"]
    output:
        bam="results/aligned/{sample}.bam",
        bai="results/aligned/{sample}.bam.bai"
    envmodules:
        "bwa/0.7.15",
        "samtools/1.14"
    conda:
        "workflow/envs/RTEnv.yml"
    log: "results/logs/snakelogs/align_reads_with_bwamem.{sample}.log"
    shell:
        """
        bwa mem -M -t {params.threads} {params.genome} {input.R1} {input.R2} | samtools sort -@ {params.threads} > {output.bam}
        samtools index -@ {params.threads} {output.bam} > {output.bai}
        """

##################################################################
##                     Mark duplicate reads                     ##
##################################################################

rule mark_duplicates_with_picard:
    input:
        bam="results/aligned/{sample}.bam"
    params:
        memory=config["picard_memory"]
    output:
        marked_bam="results/duplicatesMarkedBam/{sample}.bam",
        marked_bai="results/duplicatesMarkedBam/{sample}.bam.bai",
        marked_metrics="results/logs/picard_metrics/{sample}.marked.duplicates_metrics.txt"
    envmodules:
        "samtools/1.14",
        "picard/2.21.2"
    conda:
        "workflow/envs/RTEnv.yml"
    log: "results/logs/snakelogs/mark_duplicates_with_picard.{sample}.log"
    shell:
        """
        JAVA_MEM_OPTS={params.memory}
        RAM=$(echo $JAVA_MEM_OPTS | rev | cut -c2- | rev)
        picard MarkDuplicates COMPRESSION_LEVEL=9 VALIDATION_STRINGENCY=LENIENT MAX_RECORDS_IN_RAM=$((200000*RAM)) CREATE_INDEX=true I={input.bam} O={output.marked_bam} M={output.marked_metrics}
        samtools index {output.marked_bam}
        """

##################################################################
##              Filter .bam files based on quality              ##
##################################################################

rule quality_filter_with_bamtools:
    input:
        marked_bam="results/duplicatesMarkedBam/{sample}.bam"
    params:
        mapQ=config["bamtools_filter_mapQ"]
    output:
        filtered_bam="results/filteredBams/{sample}.bam"
        #filtered_bai="results/filteredBams/{sample}.bam.bai"
    envmodules:
        "samtools/1.14",
        "bamtools/2.5.1"
    conda:
        "workflow/envs/RTEnv.yml"
    log: "results/logs/snakelogs/quality_filter_with_bamtools.{sample}.log"
    shell:
        """
        bamtools filter -in {input.marked_bam} -forceCompression -mapQuality '{params.mapQ}' -isDuplicate false -isFailedQC false -isMapped true -isMateMapped true -isPaired true -isPrimaryAlignment true -isProperPair true -out {output.filtered_bam}
        samtools index {output.filtered_bam}
        """

##################################################################
##         Remove reads overlapping blacklisted regions         ##
##################################################################

rule blacklist_filter_with_bedtools:
    input:
        filtered_bam="results/filteredBams/{sample}.bam"
    params:
        HiCount_BED_FILENAME=config["bedtools_intersect_greylist"],
        Sat_BED_FILENAME=config["bedtools_intersect_blacklist"]
    output:   
        doubleFiltered_bam="results/doubleFilteredBams/{sample}.bam"
    envmodules:
        "samtools/1.14",
        "bedtools/2.30.0"
    conda:
        "workflow/envs/RTEnv.yml"
    log: "results/logs/snakelogs/blacklist_filter_with_bedtools.{sample}.log"
    shell:
        """
        bedtools intersect -v -split -abam {input.filtered_bam} -b {params.HiCount_BED_FILENAME} > {input.filtered_bam}_FILTERED1.BAM
        bedtools intersect -v -split -f 0.3 -abam {input.filtered_bam}_FILTERED1.BAM -b {params.Sat_BED_FILENAME} > {output.doubleFiltered_bam}
        rm {input.filtered_bam}_FILTERED1.BAM
        samtools index {output.doubleFiltered_bam}
        """

#################################################################
##                  Count reads in RT windows                  ##
#################################################################

rule count_reads_in_RT_windows:
    input:
        doubleFiltered_bam="results/doubleFilteredBams/{sample}.bam"
    params:
        RT_windows=config["RT_windows"]
    output:
        counts="results/counts/{sample}_counts.bedgraph"
    envmodules:
        "R/4.1.2-mkl",
        "bioconductor/3.14"
    conda:
        "workflow/envs/RTEnv.yml"
    log: "results/logs/snakelogs/count_reads_in_RT_windows.{sample}.log"
    shell:
        """
        Rscript workflow/scripts/CountReadsOverBed_ver02.R {params.RT_windows} {input.doubleFiltered_bam} {output.counts}
        """

#################################################################
##                      Merge count tables                     ##
#################################################################

def bedgraph_inputs(wildcards):
    files = expand("results/counts/{samples}_counts.bedgraph", samples = samples_table.index)
    return files

rule merge_count_tables:
        input:
            bedgraph_inputs
        output:
            merged="results/merged/{sample}_counts.txt",
            rse_counts="results/rse/{sample}_rse.rds"
        params:
            sample_list=' '.join(expand("{samples}",samples=samples_table.index)),
            counts_bedgraphs_list=' '.join(expand("results/counts/{samples}_counts.bedgraph",samples=samples_table.index)),
            counts_temp_list=' '.join(expand("results/temp_merge/{samples}_counts_temp.txt",samples=samples_table.index)),
            samples_table=config["samples_table"],
            RT_windows=config["RT_windows"]
        envmodules:
            "R/4.1.2-mkl",
            "bioconductor/3.14"
        conda:
            "workflow/envs/RTEnv_R.yml"
        log: "results/logs/snakelogs/{sample}_merge_count_tables.log"
        shell:
            """
            SAMPLES=( {params.sample_list} )
            BEDGRAPHS=( {params.counts_bedgraphs_list} )
            TEMP_COUNTS=( {params.counts_temp_list} )
            mkdir results/temp_merge
            for i in ${{!BEDGRAPHS[@]}}; do 
                echo -e "\t\t\t${{SAMPLES[i]}}" > ${{TEMP_COUNTS[i]}}.header
                cut -f4 ${{BEDGRAPHS[i]}} > ${{TEMP_COUNTS[i]}}.counts
                cat ${{TEMP_COUNTS[i]}}.header ${{TEMP_COUNTS[i]}}.counts > ${{TEMP_COUNTS[i]}}
                rm ${{TEMP_COUNTS[i]}}.header
                rm ${{TEMP_COUNTS[i]}}.counts
            done 
            paste {params.counts_temp_list} > {output.merged}
            rm -rf results/temp_merge/
            Rscript workflow/scripts/MakeCountsRSE_ver01.R {output.merged} {params.samples_table} {params.RT_windows} {output.rse_counts}
            """

###################################################################
##                     Process count tables                      ##
###################################################################

rule process_count_tables:
        input:
            rse_counts="results/rse/{sample}_rse.rds"
        output:
            rse_processed="results/processed_rse/{sample}_processed_rse.rds"
        envmodules:
            "R/4.1.2-mkl",
            "bioconductor/3.14"
        conda:
            "workflow/envs/RTEnv_R.yml"
        log: "results/logs/snakelogs/{sample}_process_count_tables.log"
        shell:
            """
            Rscript workflow/scripts/CalculateQuotientsSmoothScale_ver01.R {input.rse_counts} {output.rse_processed}
            """

#################################################################
##                       Make bedgraphs                        ##
#################################################################

rule make_bedgraphs:
        input:
            rse_processed="results/processed_rse/{sample}_processed_rse.rds"
        output:
            Log2Ratios_bedgraphs="results/bedgraphs/{sample}_Log2Ratios_bedgraphs.gz",
            ZScores_bedgraphs="results/bedgraphs/{sample}_ZScores_bedgraphs.gz",
            Smoothed_bedgraphs="results/bedgraphs/{sample}_Smoothed_bedgraphs.gz",
            Quotients_bedgraphs="results/bedgraphs/{sample}_Quotients_bedgraphs.gz"
        envmodules:
            "R/4.1.2-mkl",
            "bioconductor/3.14"
        conda:
            "workflow/envs/RTEnv_R.yml"
        log: "results/logs/snakelogs/{sample}_make_bedgraphs.log"
        shell:
            """
            mkdir Log2Ratios
            mkdir ZScores
            mkdir Smoothed
            mkdir Quotients
            Rscript workflow/scripts/Generate_RT_Bedgraphs_ver01.R {input.rse_processed} "Log2Ratios"
            Rscript workflow/scripts/Generate_RT_Bedgraphs_ver01.R {input.rse_processed} "ZScores"
            Rscript workflow/scripts/Generate_RT_Bedgraphs_ver01.R {input.rse_processed} "Smoothed"
            Rscript workflow/scripts/Generate_RT_Bedgraphs_ver01.R {input.rse_processed} "Quotients"
            tar -czvf {output.Log2Ratios_bedgraphs} Log2Ratios/
            tar -czvf {output.ZScores_bedgraphs} ZScores/
            tar -czvf {output.Smoothed_bedgraphs} Smoothed/
            tar -czvf {output.Quotients_bedgraphs} Quotients/
            rm -rf Log2Ratios
            rm -rf ZScores
            rm -rf Smoothed
            rm -rf Quotients
            """
